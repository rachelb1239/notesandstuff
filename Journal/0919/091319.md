# Memory Management

Tried to write 10,000,000 records to a csv. Something I'm doing is not correct (maybe not clearing the buffer right?) and my computer is on fire.

# Links
- Garbage collection and memory leaks: https://blog.codeship.com/understanding-garbage-collection-in-node-js/

## Attempt 1
- Try declaring file buffer outside of loop and clearing after each write
- Try generating single file instead of all 4 to isolate
- Result: One file writes in 1:25, let's try two

## Attempt 2
- Exactly the same as last time, but writing two files
- Result: Two file writes in 2:56

## Attempt 3
- Exactly the same as last time, but writing three files
- Result: Writing the file for reviews was taking so long that I killed the process
thinking it was stuck

## Attempt 389
- Pulled all file creation scripts into individual files
- Changed max number of writes to 200, max number of lines to 50,000
- Process was probably never hung, just taking a long time. Patience is a virtue.
- Result: Takes ~ 15 minutes to write 4 files with 10,000,000 records each

